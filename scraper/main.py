"""
SiteGround Scraper Bot - Main Entry Point

This script orchestrates the scraping process:
1. Login to SiteGround and get website list
2. For each website, scrape contact information
3. Create/update customers in CRM

Usage:
    python main.py                     # Full scrape (SiteGround + websites + CRM)
    python main.py --test-login        # Test SiteGround login only
    python main.py --siteground-only   # Scrape SiteGround only (no website scraping)
    python main.py --dry-run           # Scrape but don't update CRM
    python main.py --details           # Get detailed info from Site Tools for each site
"""
import argparse
import json
import sys
from datetime import datetime

from siteground_scraper import SiteGroundScraper
from website_scraper import WebsiteScraper
from crm_client import CRMClient, prepare_customer_data
from config import REQUEST_DELAY


def print_banner():
    """Print application banner"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           SiteGround Scraper Bot v1.0                         â•‘
â•‘           CRM MÃ¼ÅŸteri Veri Ã‡ekme AracÄ±                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)


def save_results(data: dict, filename: str = None):
    """Save results to JSON file"""
    if not filename:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'scrape_results_{timestamp}.json'
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ SonuÃ§lar {filename} dosyasÄ±na kaydedildi")
    return filename


def main():
    parser = argparse.ArgumentParser(description='SiteGround Scraper Bot')
    parser.add_argument('--test-login', action='store_true', 
                        help='Test SiteGround login only')
    parser.add_argument('--siteground-only', action='store_true',
                        help='Scrape SiteGround only, skip website scraping')
    parser.add_argument('--dry-run', action='store_true',
                        help='Scrape data but do not update CRM')
    parser.add_argument('--details', action='store_true',
                        help='Get detailed info from Site Tools for each site')
    parser.add_argument('--output', type=str, default=None,
                        help='Output JSON file path')
    parser.add_argument('--limit', type=int, default=None,
                        help='Limit number of sites to process')
    
    args = parser.parse_args()
    
    print_banner()
    
    # Test login mode
    if args.test_login:
        print("ğŸ” SiteGround giriÅŸ testi yapÄ±lÄ±yor...")
        scraper = SiteGroundScraper()
        success = scraper.test_login()
        sys.exit(0 if success else 1)
    
    # Initialize results
    results = {
        'timestamp': datetime.now().isoformat(),
        'siteground_sites': [],
        'website_data': [],
        'crm_updates': [],
        'errors': [],
    }
    
    # Step 1: Scrape SiteGround
    print("\n" + "="*60)
    print("ADIM 1: SiteGround'dan website listesi Ã§ekiliyor")
    print("="*60)
    
    sg_scraper = SiteGroundScraper()
    
    # Start browser for SiteGround - we'll reuse this for website scraping too
    sg_scraper.start_browser()
    
    try:
        if not sg_scraper.login():
            print("âŒ SiteGround'a giriÅŸ yapÄ±lamadÄ±!")
            save_results(results, args.output)
            sys.exit(1)
        
        sg_scraper.navigate_to_websites()
        siteground_sites = sg_scraper.get_websites_list()
        results['siteground_sites'] = siteground_sites
        
        if not siteground_sites:
            print("âŒ SiteGround'dan site listesi Ã§ekilemedi!")
            save_results(results, args.output)
            sys.exit(1)
        
        print(f"\nâœ… {len(siteground_sites)} site bulundu")
        
        # Apply limit if specified
        sites_to_process = siteground_sites
        if args.limit:
            sites_to_process = siteground_sites[:args.limit]
            print(f"ğŸ“Š {args.limit} site ile sÄ±nÄ±rlandÄ±rÄ±ldÄ±")
        
        # Step 2: Scrape websites using the SAME browser session
        # This may help bypass bot protection since we're already authenticated
        website_data = {}
        
        if not args.siteground_only:
            print("\n" + "="*60)
            print("ADIM 2: Website iletiÅŸim bilgileri Ã§ekiliyor")
            print("="*60)
            print("   (AynÄ± tarayÄ±cÄ± oturumu kullanÄ±lÄ±yor - bot korumasÄ±nÄ± atlamak iÃ§in)")
            
            # Use the existing page from SiteGround scraper
            from website_scraper import WebsiteScraper
            web_scraper = WebsiteScraper(use_existing_browser=True, page=sg_scraper.page)
            
            for i, site in enumerate(sites_to_process):
                domain = site.get('domain', '')
                if not domain:
                    continue
                
                print(f"\n[{i+1}/{len(sites_to_process)}]")
                scraped = web_scraper.scrape_website(domain)
                website_data[domain] = scraped
                results['website_data'].append(scraped)
            
            print(f"\nâœ… {len(website_data)} website tarandÄ±")
        else:
            print("\nâ­ï¸ Website tarama atlandÄ± (--siteground-only modu)")
    
    finally:
        sg_scraper.close_browser()
    
    # Step 3: Update CRM (unless dry-run mode)
    if not args.dry_run:
        print("\n" + "="*60)
        print("ADIM 3: CRM gÃ¼ncelleniyor")
        print("="*60)
        
        crm = CRMClient()
        
        if not crm.test_connection():
            print("âŒ CRM baÄŸlantÄ±sÄ± kurulamadÄ±!")
            results['errors'].append('CRM connection failed')
            save_results(results, args.output)
            sys.exit(1)
        
        for site in sites_to_process:
            domain = site.get('domain', '')
            if not domain:
                continue
            
            print(f"\nğŸ”„ {domain} iÅŸleniyor...")
            
            try:
                # Prepare customer data
                sg_data = site
                web_data = website_data.get(domain, {})
                customer_data = prepare_customer_data(domain, sg_data, web_data)
                
                # Create or update customer
                result = crm.create_or_update_customer(domain, customer_data)
                results['crm_updates'].append({
                    'domain': domain,
                    'success': True,
                    'customer_id': result.get('id'),
                })
                print(f"   âœ… MÃ¼ÅŸteri gÃ¼ncellendi: ID {result.get('id')}")
                
            except Exception as e:
                error_msg = str(e)
                results['crm_updates'].append({
                    'domain': domain,
                    'success': False,
                    'error': error_msg,
                })
                results['errors'].append(f"{domain}: {error_msg}")
                print(f"   âŒ Hata: {error_msg[:100]}")
        
        # Summary
        successful = sum(1 for u in results['crm_updates'] if u['success'])
        failed = len(results['crm_updates']) - successful
        print(f"\nâœ… {successful} mÃ¼ÅŸteri baÅŸarÄ±yla gÃ¼ncellendi")
        if failed:
            print(f"âŒ {failed} hata oluÅŸtu")
    else:
        print("\nâ­ï¸ CRM gÃ¼ncelleme atlandÄ± (--dry-run modu)")
    
    # Save results
    save_results(results, args.output)
    
    # Final summary
    print("\n" + "="*60)
    print("Ã–ZET")
    print("="*60)
    print(f"ğŸ“Š SiteGround siteleri: {len(siteground_sites)}")
    print(f"ğŸŒ Taranan websiteler: {len(website_data)}")
    print(f"ğŸ‘¥ CRM gÃ¼ncellemeleri: {len(results['crm_updates'])}")
    if results['errors']:
        print(f"âš ï¸ Hatalar: {len(results['errors'])}")
    
    print("\nâœ¨ Ä°ÅŸlem tamamlandÄ±!")


if __name__ == '__main__':
    main()
